{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7d/41zlxmgs4j10fhgnr31g8mnw0000gn/T/ipykernel_20760/1507330949.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['question1'].fillna('', inplace=True)\n",
      "/var/folders/7d/41zlxmgs4j10fhgnr31g8mnw0000gn/T/ipykernel_20760/1507330949.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['question2'].fillna('', inplace=True)\n",
      "/var/folders/7d/41zlxmgs4j10fhgnr31g8mnw0000gn/T/ipykernel_20760/1507330949.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_data['question1'].fillna('', inplace=True)\n",
      "/var/folders/7d/41zlxmgs4j10fhgnr31g8mnw0000gn/T/ipykernel_20760/1507330949.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_data['question2'].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "#Since mac sucks use a subset\n",
    "train_data = train_data.sample(1000, random_state=42)\n",
    "test_data = test_data.sample(1000, random_state=42)\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "train_data['question1'].fillna('', inplace=True)\n",
    "train_data['question2'].fillna('', inplace=True)\n",
    "test_data['question1'].fillna('', inplace=True)\n",
    "test_data['question2'].fillna('', inplace=True)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', str(text))  # Remove non-word characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text.strip()  # Remove leading/trailing whitespace\n",
    "\n",
    "# Apply preprocessing\n",
    "train_data['question1'] = train_data['question1'].apply(preprocess_text)\n",
    "train_data['question2'] = train_data['question2'].apply(preprocess_text)\n",
    "test_data['question1'] = test_data['question1'].apply(preprocess_text)\n",
    "test_data['question2'] = test_data['question2'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- Removing non-word characters using regular expressions.\n",
    "- Converting text to lowercase for uniformity.\n",
    "- Splitting the text into tokens (words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Sentence** Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This class loads the SBERT model, which is pre-trained to generate embeddings that capture the semantic meaning of sentences.\n",
    "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def encode_sentence(sentence, model):\n",
    "    if sentence == '':\n",
    "        print(\"Empty sentence encountered, returning zero vector.\")\n",
    "        return np.zeros(384) \n",
    "    return model.encode(sentence)\n",
    "# If the sentence is not empty, it uses the SBERT model to generate a 384-dimensional embedding that captures the sentence's semantic meaning.\n",
    "\n",
    "\n",
    "train_data['q1_sbert'] = train_data['question1'].apply(lambda x: encode_sentence(x, sbert_model))\n",
    "train_data['q2_sbert'] = train_data['question2'].apply(lambda x: encode_sentence(x, sbert_model))\n",
    "test_data['q1_sbert'] = test_data['question1'].apply(lambda x: encode_sentence(x, sbert_model))\n",
    "test_data['q2_sbert'] = test_data['question2'].apply(lambda x: encode_sentence(x, sbert_model))\n",
    "\n",
    "# Calculate cosine similarity between SBERT embeddings\n",
    "train_data['sbert_sim'] = train_data.apply(lambda row: cosine_similarity([row['q1_sbert']], [row['q2_sbert']])[0][0], axis=1)\n",
    "test_data['sbert_sim'] = test_data.apply(lambda row: cosine_similarity([row['q1_sbert']], [row['q2_sbert']])[0][0], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Calculate Cosine Similarity?**\n",
    "\n",
    "Measure Semantic Similarity: Cosine similarity quantifies how close two vectors are in the high-dimensional space. For sentence embeddings, this translates to how similar the meanings of two sentences are. A higher cosine similarity indicates that the sentences are more similar in meaning.\n",
    "\n",
    "Feature for Classification: In the context of the Quora question pairs problem, the cosine similarity between the embeddings of two questions can be used as a feature to help a machine learning model determine whether the questions are duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output Shape: When cosine_similarity is called with these inputs, the result is a 1x1 matrix, where the only entry represents the cosine similarity between the single vector in the first list and the single vector in the second list.\n",
    "- Extracting the Scalar Value: To get this single similarity value, you need to extract the first (and only) element from this 1x1 matrix. This is achieved with [0][0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all questions for TF-IDF vectorization\n",
    "all_questions = pd.concat([train_data['question1'], train_data['question2'], test_data['question1'], test_data['question2']], axis=0)\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=10000)\n",
    "tfidf.fit(all_questions)\n",
    "\n",
    "train_q1_tfidf = tfidf.transform(train_data['question1'])\n",
    "train_q2_tfidf = tfidf.transform(train_data['question2'])\n",
    "test_q1_tfidf = tfidf.transform(test_data['question1'])\n",
    "test_q2_tfidf = tfidf.transform(test_data['question2'])\n",
    "\n",
    "# Create feature vectors by taking the absolute difference of TF-IDF vectors\n",
    "train_features_tfidf = abs(train_q1_tfidf - train_q2_tfidf)\n",
    "test_features_tfidf = abs(test_q1_tfidf - test_q2_tfidf)\n",
    "\n",
    "# Calculate number of common words and normalize by total unique words\n",
    "\n",
    "def common_words(q1, q2):\n",
    "    return len(set(q1.split()) & set(q2.split()))\n",
    "\n",
    "train_data['common_words'] = train_data.apply(lambda row: common_words(row['question1'], row['question2']), axis=1)\n",
    "test_data['common_words'] = test_data.apply(lambda row: common_words(row['question1'], row['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Words:** \\\n",
    "Purpose: Measures the number of words that are shared between the two questions.\\\n",
    "Meaning: A higher number of common words indicates that the questions might be asking about the same topic.\n",
    "\n",
    "**Common Word Ratio:** \\\n",
    "Purpose: Normalizes the common word count by the total number of unique words in both questions.\\\n",
    "Meaning: This ratio gives a normalized measure of word overlap, accounting for the length of the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features\n",
    "train_features = pd.DataFrame()\n",
    "train_features['sbert_sim'] = train_data['sbert_sim']\n",
    "train_features['common_words'] = train_data['common_words']\n",
    "# train_features['common_word_ratio'] = train_data['common_word_ratio']\n",
    "\n",
    "test_features = pd.DataFrame()\n",
    "test_features['sbert_sim'] = test_data['sbert_sim']\n",
    "test_features['common_words'] = test_data['common_words']\n",
    "# test_features['common_word_ratio'] = test_data['common_word_ratio']\n",
    "\n",
    "# Add TF-IDF features\n",
    "tfidf_train_features_df = pd.DataFrame(train_features_tfidf.toarray())\n",
    "tfidf_test_features_df = pd.DataFrame(test_features_tfidf.toarray())\n",
    "\n",
    "train_features = pd.concat([train_features.reset_index(drop=True), tfidf_train_features_df.reset_index(drop=True)], axis=1)\n",
    "test_features = pd.concat([test_features.reset_index(drop=True), tfidf_test_features_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Prepare feature matrix and target variable\n",
    "X = train_features\n",
    "y = train_data['is_duplicate']\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Log Loss (XGBoost): 0.4156219731899407\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "val_predictions_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
    "print(f'Validation Log Loss (XGBoost): {log_loss(y_val, val_predictions_xgb)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Validation Log Loss (Neural Network): 0.6259429905183549\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Define the improved neural network model\n",
    "nn_model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(), \n",
    "    Dropout(0.4), \n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\"\"\"\n",
    "Explanation of neural network structure:\n",
    "- Dense Layers: Each Dense layer is a fully connected layer where every neuron is connected to every neuron in the next layer.\n",
    "- Activation Function: relu (Rectified Linear Unit) is used for hidden layers because it helps with non-linearity and is computationally efficient.\n",
    "- Batch Normalization: Added after each hidden layer to normalize the inputs to each layer.\n",
    "- Dropout: Set to 40% to ignore 40% of the neurons during training, which helps prevent overfitting.\n",
    "- Output Layer: Uses sigmoid activation for binary classification, producing a probability between 0 and 1.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add callbacks for better training\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "\"\"\"\n",
    "- ReduceLROnPlateau: Reduces the learning rate by a factor of 0.2 if the validation loss does not improve for 3 epochs, with a minimum learning rate of 0.001.\n",
    "- EarlyStopping: Stops training if the validation loss does not improve for 5 epochs.\n",
    "- ModelCheckpoint: Saves the model with the best validation loss during training.\n",
    "\"\"\"\n",
    "\n",
    "# Train the neural network model\n",
    "nn_model.fit(X_train, y_train, epochs=50, batch_size=256, validation_data=(X_val, y_val),\n",
    "             callbacks=[reduce_lr, early_stop, model_checkpoint],verbose = 0)\n",
    "\n",
    "# Load the best model\n",
    "nn_model.load_weights('best_model.keras')\n",
    "\n",
    "# Validate the model\n",
    "val_predictions_nn = nn_model.predict(X_val).flatten()\n",
    "print(f'Validation Log Loss (Neural Network): {log_loss(y_val, val_predictions_nn)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are Callbacks?\n",
    "\n",
    "Callbacks are functions that can be applied at different stages of the training process. They are used to customize the behavior of the training loop.\n",
    "\n",
    "Why use Callbacks?\n",
    "\n",
    "- Reduce Learning Rate on Plateau: The ReduceLROnPlateau callback reduces the learning rate when the validation loss stops improving. This helps the model converge better by taking smaller steps during training.\n",
    "- Early Stopping: The EarlyStopping callback stops training when the validation loss stops improving. This prevents the model from overfitting by stopping training at the optimal point.\n",
    "- Model Checkpointing: The ModelCheckpoint callback saves the best model during training based on validation loss. This ensures that you keep the best-performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochs and Batch Size:\n",
    "- Exploration Phase: Initially, we set 50 epochs to allow the model enough time to learn from the data. However, we use the EarlyStopping callback to prevent overfitting by stopping the training early if the validation loss does not improve.\n",
    "- Early Stopping: This means the model will likely not train for all 50 epochs. Instead, it will stop as soon as the validation loss stops improving, making the number of epochs more flexible and dependent on the data and the model's learning behavior.\n",
    "\n",
    "What is Batch Size?\n",
    "\n",
    "The batch size is the number of training samples used to compute the gradient update during training. It defines how many samples to use before updating the model's internal parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using XGBoost model\n",
    "test_predictions_xgb = xgb_model.predict_proba(test_features)[:, 1]\n",
    "\n",
    "# Make predictions using Neural Network model\n",
    "test_predictions_nn = nn_model.predict(test_features).flatten()\n",
    "\n",
    "# Average predictions\n",
    "test_predictions = (test_predictions_xgb + test_predictions_nn) / 2\n",
    "\n",
    "# Prepare submission file\n",
    "submission = pd.DataFrame({'test_id': test_data['test_id'], 'is_duplicate': test_predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission = pd.DataFrame({'test_id': test_data['test_id'], 'is_duplicate': test_predictions_xgb})\n",
    "submission.to_csv('submission_xgb.csv', index=False)\n",
    "submission = pd.DataFrame({'test_id': test_data['test_id'], 'is_duplicate': test_predictions_nn})\n",
    "submission.to_csv('submission_nn.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
